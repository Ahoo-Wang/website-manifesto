@main("Glossary - The Reactive Manifesto") {
    <main>
        <header>
            <a class="title-link" href="/"><h1>リアクティブ宣言 用語集</h1></a>
        </header>

        <article>
            <section id="glossary">
                <h1>Glossary</h1>
                <div class="toc cf">
                    <h2>Table of Contents</h2>
                    <ul>
                        <li><a href="#Asynchronous">非同期</a></li>
                        <li><a href="#Back-Pressure">バック・プレッシャー</a></li>
                        <li><a href="#Batching">バッチ処理</a></li>
                        <li><a href="#Delegation">委譲</a></li>
                        <li><a href="#Component">コンポーネント</a></li>
                        <li><a href="#Elasticity">弾力性（スケーラビリティと対比して</a></li>
                        <li><a href="#Failure">障害（エラーと対比して</a></li>
                        <li><a href="#Isolation">隔離（と封じ込め</a></li>
                        <li><a href="#Location-Transparency">位置透過性</a></li>
                    </ul>
                    <ul>
                        <li><a href="#Message-Driven">メッセージ駆動（イベント駆動と対比して</a></li>
                        <li><a href="#Non-Blocking">ノンブロッキング</a></li>
                        <li><a href="#Protocol">プロトコル</a></li>
                        <li><a href="#Replication">レプリケーション</a></li>
                        <li><a href="#Resource">リソース</a></li>
                        <li><a href="#Scalability">スケーラビリティ</a></li>
                        <li><a href="#System">システム</a></li>
                        <li><a href="#User">ユーザ</a></li>
                    </ul>
                </div>


                <h2 id="Asynchronous"><a href="#Asynchronous" class="link-icon">@image("link-icon.png")</a>非同期</h2>
                <p>オックスフォード英語辞典は、非同期 (asynchronous) を「同時に存在したり起きたりしないこと」と定義している。リアクティブ宣言の文脈では、「クライアントからサービスへ送信されたリクエストが、送信後の任意の時点で処理されること」を意味する。送信先のサービス内でのリクエスト処理の実行を直接クライアントが観測したり、それに対して同期を取ることはできない。非同期の対義語である同期処理では、サービスがリクエストを処理するまでクライアントは自身の実行を再開しない。</p>

                <h2 id="Back-Pressure"><a href="#Back-Pressure" class="link-icon">@image("link-icon.png")</a>バック・プレッシャー</h2>
                <p>ある<a href="#Component">コンポーネント</a>が全体に追いつけなくなった場合、<a href="#System">システム</a>全体として何らかの対処をする必要がある。過負荷状態のコンポーネントが壊滅的にクラッシュしたり、制御無くメッセージを損失することは許されない。処理が追いつかなくなっていて、かつクラッシュすることも許されないならば、コンポーネントは上流のコンポーネント群に自身が過負荷状態であることを伝えて負荷を減らしてもらうべきだ。この**バック・プレッシャー** (back-pressure) と呼ばれる仕組みは、過負荷の下でシステムを崩壊させず緩やかに応答を続ける重要なフィードバック機構だ。バック・プレッシャーはユーザまで転送してもよく、その場合、即応性 (resilient) は低下するが負荷の下でのシステムの耐障害性が保証される。また、システムがその情報を使って自身に他のリソースを振り向け、負荷分散を促すこともできる。<a href="#Elasticity">弾力性</a>を参照。</p>

                <h2 id="Batching"><a href="#Batching" class="link-icon">@image("link-icon.png")</a>バッチ処理</h2>
                <p>現在のコンピュータは同じタスクを繰り返し実行することに最適化されている。命令キャッシュや分岐予測によって、クロック周波数を一定に保ったまま一秒間に処理できる命令数を増加することができるためだ。そのため、同じ CPU コアへ異なるタスクを立て続けに与えてしまうと、この最適化によって得られる性能をフルに引き出すことができない。もし可能ならば、異なるタスクを交互に実行する頻度を少なくするようにプログラムを構成するべきだ。つまり、データ要素を一つの集合にしてバッチ で処理したり、異なる処理ステップを専用のハードウェアスレッドで実行したりすればよい。</p>

                <p>同期や協調を必要とする外部<a href="#Resource">リソース</a>を使用する際も、同様の論理が当てはまる。永続的ストレージ機器が提供する I/O 帯域幅は、単一のスレッド（と CPU コア）がコマンドを発行することにより、全てのコアに帯域幅を争わせる場合よりも劇的に改善する。この方法のさらなる利点は、機器へのエントリポイントが単一なので、機器にとって最適なアクセスパターンへより適合するように操作の順序を入れ替えられることだ。</p>
                <p>さらに、バッチ処理により I/O のような高価な操作や高価な計算のコストを分配できることがある。例えば、複数のデータ要素を同じネットワークパケットやディスクブロックにまとめることで、効率の向上と利用の削減ができる</p>


                <h2 id="Delegation"><a href="#Delegation" class="link-icon">@image("link-icon.png")</a>Delegation</h2>
                <p>Delegating a task asynchronously to another component means that the execution of the task will take place in the context of that other component. This delegated context could entail running in a different error handling context, on a different thread, in a different process, or on a different network node, to name a few possibilities. The purpose of delegation is to hand over the processing responsibility of a task to another component so that the delegating component can perform other processing or optionally observe the progress of the delegated task in case additional action is required such as handling failure or reporting progress.</p>

                <h2 id="Component"><a href="#Component" class="link-icon">@image("link-icon.png")</a>Component</h2>
                <p>What we are describing is a modular software architecture, which is a very old idea, see for example <a href="https://www.cs.umd.edu/class/spring2003/cmsc838p/Design/criteria.pdf" target="_blank">Parnas (1972)</a>. We are using the term “component” due to its proximity with compartment, which implies that each component is self-contained, encapsulated and isolated from other components. This notion applies foremost to the runtime characteristics of the system, but it will typically also be reflected in the source code’s module structure as well. While different components might make use of the same software modules to perform common tasks, the program code that defines the top-level behavior of each component is then a module of its own. Component boundaries are often closely aligned with <a href="http://martinfowler.com/bliki/BoundedContext.html" target="_blank">Bounded Contexts</a> in the problem domain. This means that the system design tends to reflect the problem domain and so is easy to evolve, while retaining isolation. Message protocols provide a natural mapping and communications layer between Bounded Contexts (components).</p>

                <h2 id="Elasticity"><a href="#Elasticity" class="link-icon">@image("link-icon.png")</a>Elasticity (in contrast to Scalability)</h2>
                <p>Elasticity means that the throughput of a system scales up or down automatically to meet varying demand as resource is proportionally added or removed. The system needs to be scalable (see <a href="#Scalability">Scalability</a>) to allow it to benefit from the dynamic addition, or removal, of resources at runtime. Elasticity therefore builds upon scalability and expands on it by adding the notion of automatic <a href="#Resource">resource</a> management.</p>

                <h2 id="Failure"><a href="#Failure" class="link-icon">@image("link-icon.png")</a>Failure (in contrast to Error)</h2>
                <p>A failure is an unexpected event within a service that prevents it from continuing to function normally. A failure will generally prevent responses to the current, and possibly all following, client requests. This is in contrast with an error, which is an expected and coded-for condition—for example an error discovered during input validation, that will be communicated to the client as part of the normal processing of the message. Failures are unexpected and will require intervention before the system can resume at the same level of operation. This does not mean that failures are always fatal, rather that some capacity of the system will be reduced following a failure. Errors are an expected part of normal operations, are dealt with immediately and the system will continue to operate at the same capacity following an error.</p>
                <p>Examples of failures are hardware malfunction, processes terminating due to fatal resource exhaustion, program defects that result in corrupted internal state.</p>

                <h2 id="Isolation"><a href="#Isolation" class="link-icon">@image("link-icon.png")</a>Isolation (and Containment)</h2>
                <p>Isolation can be defined in terms of decoupling, both in time and space. Decoupling in time means that the sender and receiver can have independent life-cycles—they do not need to be present at the same time for communication to be possible. It is enabled by adding asynchronous boundaries between the components, communicating through message-passing. Decoupling in space (defined as Location Transparency) means that the sender and receiver do not have to run in the same process, but wherever the operations division or the runtime itself decides is most efficient—which might change during an application's lifetime. </p>
                <p>True isolation goes beyond the notion of encapsulation found in most object-oriented languages and gives us compartmentalization and containment of:</p>
                <ul>
                    <li>State and behavior: it enables share-nothing designs and minimizes contention and coherence cost (as defined in the <a href="http://www.perfdynamics.com/Manifesto/USLscalability.html" target="_blank"> Universal Scalability Law</a>); </li>
                    <li>Failures: it allows <a href="#Failure">errors</a> to be captured, signalled and managed at a fine-grained level instead of letting them cascade to other components.</li>
                </ul>
                <p>Strong isolation between components is built on communication over well-defined <a href="#Protocol">protocols</a> and enables loose coupling, leading to systems that are easier to understand, extend, test and evolve.</p>

                <h2 id="Location-Transparency"><a href="#Location-Transparency" class="link-icon">@image("link-icon.png")</a>Location Transparency</h2>
                <p>Elastic systems needs to be adaptive and continuously react to changes in demand, they need to gracefully and efficiently increase and decrease scale. One key insight that simplifies this problem immensely is to realize that we are all doing distributed computing. This is true whether we are running our systems on a single node (with multiple independent CPUs communicating over the QPI link) or on a cluster of nodes (with independent machines communicating over the network). Embracing this fact means that there is no conceptual difference between scaling vertically on multicore or horizontally on the cluster.</p>
                <p>If all of our <a href="#Component">components</a> support mobility, and local communication is just an optimization, then we do not have to define a static system topology and deployment model upfront. We can leave this decision to the operations personnel and the runtime, which can adapt and optimize the system depending on how it is used.</p>
                <p>This decoupling in space (see the the definition for Isolation), enabled through asynchronous message-passing, and decoupling of the runtime instances from their references is what we call Location Transparency. Location Transparency is often mistaken for 'transparent distributed computing', while it is actually the opposite: we embrace the network and all its constraints—like partial failure, network splits, dropped messages, and its asynchronous and message-based nature—by making them first class in the programming model, instead of trying to emulate in-process method dispatch on the network (ala RPC, XA etc.). Our view of Location Transparency is in perfect agreement with <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628" target="_blank"> A Note On Distributed Computing</a> by Waldo et al.</p>

                <h2 id="Message-Driven"><a href="#Message-Driven" class="link-icon">@image("link-icon.png")</a>Message-Driven (in contrast to Event-Driven)</h2>
                <p>A message is an item of data that is sent to a specific destination. An event is a signal emitted by a component upon reaching a given state. In a message-driven system addressable recipients await the arrival of messages and react to them, otherwise lying dormant. In an event-driven system notification listeners are attached to the sources of events such that they are invoked when the event is emitted. This means that an event-driven system focuses on addressable event sources while a message-driven system concentrates on addressable recipients. A message can contain an encoded event as its payload.</p>
                <p>Resilience is more difficult to achieve in an event-driven system due to the short-lived nature of event consumption chains: when processing is set in motion and listeners are attached in order to react to and transform the result, these listeners typically handle success or failure directly and in the sense of reporting back to the original client. Responding to the failure of a component in order to restore its proper function, on the other hand, requires a treatment of these failures that is not tied to ephemeral client requests, but that responds to the overall component health state.</p>

                <h2 id="Non-Blocking"><a href="#Non-Blocking" class="link-icon">@image("link-icon.png")</a>Non-Blocking</h2>
                <p>In concurrent programming an algorithm is considered non-blocking if threads competing for a resource do not have their execution indefinitely postponed by mutual exclusion protecting that resource. In practice this usually manifests as an API that allows access to the resource if it is available otherwise it immediately returns informing the caller that the resource is not currently available or that the operation has been initiated and not yet completed. A non-blocking API to a resource allows the caller the option to do other work rather than be blocked waiting on the resource to become available. This may be complemented by allowing the client of the resource to register for getting notified when the resource is available or the operation has completed.</p>

                <h2 id="Protocol"><a href="#Protocol" class="link-icon">@image("link-icon.png")</a>Protocol</h2>
                <p>A protocol defines the treatment and etiquette for the exchange or transmission of messages between components. Protocols are formulated as relations between the participants to the exchange, the accumulated state of the protocol and the allowed set of messages to be sent. This means that a protocol describes which messages a participant may send to another participant at any given point in time. Protocols can be classified by the shape of the exchange, some common classes are request–reply, repeated request–reply (as in HTTP), publish–subscribe, and stream (both push and pull).</p>
                <p>In comparison to local programming interfaces a protocol is more generic since it can include more than two participants and it foresees a progression of the state of the message exchange; an interface only specifies one interaction at a time between the caller and the receiver.</p>
                <p>It should be noted that a protocol as defined here just specifies which messages may be sent, but not how they are sent: encoding, decoding (i.e. codecs), and transport mechanisms are implementation details that are transparent to the components’ use of the protocol.</p>

                <h2 id="Replication"><a href="#Replication" class="link-icon">@image("link-icon.png")</a>Replication</h2>
                <p>Executing a component simultaneously in different places is referred to as replication. This can mean executing on different threads or thread pools, processes, network nodes, or computing centers. Replication offers scalability, where the incoming workload is distributed across multiple instances of a component, or resilience, where the incoming workload is replicated to multiple instances which process the same requests in parallel. These approaches can be mixed, for example by ensuring that all transactions pertaining to a certain user of the component will be executed by two instances while the total number of instances varies with the incoming load, (see Elasticity).</p>

                <h2 id="Resource"><a href="#Resource" class="link-icon">@image("link-icon.png")</a>Resource</h2>
                <p>Everything that a component relies upon to perform its function is a resource that must be provisioned according to the component’s needs. This includes CPU allocation, main memory and persistent storage as well as network bandwidth, main memory bandwidth, CPU caches, inter-socket CPU links, reliable timer and task scheduling services, other input and output devices, external services like databases or network file systems etc. The elasticity and resilience of all these resources must be considered, since the lack of a required resource will prevent the component from functioning when required.</p>

                <h2 id="Scalability"><a href="#Scalability" class="link-icon">@image("link-icon.png")</a>Scalability</h2>
                <p>The ability of a system to make use of more computing resources in order to increase its performance is measured by the ratio of throughput gain to resource increase. A perfectly scalable system is characterized by both both numbers being proportional: a twofold allocation of resources will double the throughput. Scalability is typically limited by the introduction of bottlenecks or synchronization points within the system, leading to constrained scalability, see <a href="http://blogs.msdn.com/b/ddperf/archive/2009/04/29/parallel-scalability-isn-t-child-s-play-part-2-amdahl-s-law-vs-gunther-s-law.aspx" target="_blank"> Amdahl’s Law and Gunther’s Universal Scalability Model</a>.</p>

                <h2 id="System"><a href="#System" class="link-icon">@image("link-icon.png")</a>System</h2>
                <p>A system provides services to its users or clients. Systems can be large or small, in which case they comprise many or just a few components. All components of a system collaborate to provide these services. In many cases the components are in a client–server relationship within the same system (consider for example front-end components relying upon back-end components). A system shares a common resilience model, by which we mean that failure of a component is handled within the system, delegated from one component to the other. It is useful to view groups of components within a system as subsystems if they are isolated from the rest of the system in their function, resources or failure modes.</p>

                <h2 id="User"><a href="#User" class="link-icon">@image("link-icon.png")</a>User</h2>
                <p>We use this term informally to refer to any consumer of a service, be that a human or another service.</p>

                <a href="#glossary" class="btt">&#8593; BACK TO TOP</a>


            </section>
        </article>
        @footer()
    </main>
}
